\documentclass[main.tex]{subfiles}

\begin{document}

\section{Dataset Description}\label{sec:dataset}

\subsection{Dataset Selection}
The dataset is taken from Kaggle and was uploaded 2 years ago by user Tommaso Ramella. It contains data scraped from the website Immobiliare.it about housing announcements in Italy in the year 2023. The author provides the public with two different datasets: the first is about rentals and the second contains purchase listings. Two different versions of each are available, a raw one, consisting of the original data, and a clean one. On the page, the author states the intention to provide a notebook with the methodology applied to parse and clean the data, but we were not able to find the complete version of this notebook (a part of the process can be found in the data publisher’s GitHub folder) and as such we assume it was never released. While at the start we were inclined to just use the clean version for rents, in the end we decided against it and tried to gain more insight into the data by doing the work ourselves and trying to get as many features as we could.

The raw dataset contains around 126000 entries. Due to the number of elements in the original set, it was decided to apply machine learning techniques to a smaller subset of interest; that is, the rents in the city of Rome, a sore spot for students and a field with very practical ramifications and consequences. Our objective was to create a way for someone to get a basic understanding of the price a house should have given its characteristics, to avoid being fooled.

The records in the dataset related to Rome (\texttt{rome\_rents\_raw.csv}) are 13276, with 38 columns of information for each of them.

\subsection{Data Preprocessing}
This section outlines the preprocessing pipeline implemented to transform raw rental listings into a structured dataset suitable for analysis and modeling.

The first step in the preprocessing pipeline involves standardizing the dataset structure. All column headers in the raw dataset, originally in Italian, are renamed to English to ensure consistency and readability throughout the analysis.  Following this translation, specific cleaning operations are performed on each column to handle missing values and remove noise.

\subsubsection{Handling Missing or Noisy Data}
The raw data presented several challenges typical of web-scraped real estate listings. Numeric fields contained currency symbols, unit suffixes, and text values that needed systematic cleaning. For monetary columns like \texttt{price}, \texttt{condo\_fees}, and \texttt{deposit}, we removed currency symbols (€) and text indicators (e.g., ``/mese''), converting text representations of zero (``nessuna'', ``n.d.'') to 0.0 and parsing valid entries as floats. Similarly, \texttt{square\_meters} required stripping the m² suffix before numeric conversion.

Room and bathroom columns used notations like ``5+'' so we addressed this by extracting the base integer while creating separate boolean flags (\texttt{more\_than\_5\_rooms}, \texttt{more\_than\_3\_bathrooms}) to preserve the additional information without compromising data type integrity. The \texttt{floor} column required more sophisticated normalization, mapping Italian descriptions (ex. ground floor, basement ec...) to standardized integers (0 for ground floor, -1 for basement). We had this approach based on the hypothesis that a higher floor number could correlate with higher rental prices in Rome, due to better views and reduced street noise.

\subsubsection{Complex Feature Extraction}
Several columns column contained structured information embedded within seemingly unstructured text, requiring sophisticated parsing to extract multiple features from single fields.

The \texttt{floor} column exemplifies this complexity. Beyond the floor number itself that we already addressed, descriptions often included accessibility information such as elevator availability or disability access. We extracted some distinct boolean features from this field: elevator presence, disability accessibility, raised floor status, multi-floor properties and top floor location.

The \texttt{contract} field proved particularly rich, as Italian rental law recognizes multiple contract types with varying legal implications. We parsed nine features including contract type, minimum duration and renewal terms (often expressed as ``3+2'' for 3 years plus 2-year renewal), and specialized categories such as student rentals, transitional contracts, and buyout options.

From \texttt{rooms\_details}, we separated total room count from bedrooms and other spaces, classified kitchen types (reflecting Italian distinctions between full kitchens and kitchenettes), and identified special amenities. The \texttt{condition} field was split into overall property condition and renovation status, later one-hot encoded into binary features.

Parking information (\texttt{parking\_spaces}) was decomposed into counts for garage/box spaces, outdoor parking, common areas, and private boxes, with corresponding boolean flags for presence detection.

The \texttt{other\_features} column yielded 31 binary indicators for amenities ranging from outdoor spaces (terrace, balconies, garden, swimming pool) to security features (alarm, video intercom, security door), utilities (fiber optic, elevator), furnishing status, and property orientation (south-facing, east-facing, internal/external exposure).

Energy efficiency was handled by extracting both the categorical class (A--G, mapped to numeric values 1--7) and the consumption value in kWh/m²·year using regex patterns to accommodate various formats. The \texttt{availability} field was simplified to a boolean indicating immediate availability, while \texttt{year\_built} was cleaned and converted to an integer \texttt{construction\_year}.

Finally, the free-text \texttt{description} column was converted to structured boolean features through keyword extraction. We identified high-value indicators of location amenities (subway, train station, university, hospital, park), property characteristics (bright, quiet), fixtures (shower, bathtub, wardrobe), and nearby services (market), transforming unstructured narrative into quantifiable features that preserve semantic information about desirability and accessibility.

\subsubsection{Adding Geographic Information}
Given that listings provided only neighborhood names, we enriched the dataset with geographic context by mapping each neighborhood to three features using an external reference file: a boolean indicator for location within the GRA (Great Ring Road, \texttt{inside\_gra}), a categorical zone feature (\texttt{zone}: center, north, south, east, west), and the administrative municipality (\texttt{municipality}). This geographic stratification provides essential location context for price prediction, as rental values in Rome vary significantly by area.

\subsubsection{One-Hot Encoding}
We transformed categorical columns into comprehensive boolean feature sets to capture property characteristics systematically. 

The \texttt{property\_type} column generated 22 binary flags covering dwelling types (condominium, various villa configurations, penthouse, loft, farmhouse), luxury classifications (prestigious, middle, economical class), and ownership structures (full, partial, usufruct, timeshare).

\subsubsection{Data Splitting}
% Explain how data was split into training, validation, and test sets
% Mention the ratios used (e.g., 60-20-20 or 70-15-15)

\subsubsection{Feature Normalization}
% Describe normalization or standardization techniques applied

\subsubsection{Feature Engineering}
% If applicable, describe any dimensionality reduction (PCA) or feature selection performed

\section{Methodology}\label{sec:methodology}

\subsection{Models Implemented}
% Brief overview of all models used in the study

\subsubsection{Naïve Bayes}
% Describe the Naïve Bayes classifier (Gaussian or Multinomial)
% Explain the assumptions and theoretical background

\subsubsection{Logistic Regression}
% Describe Logistic Regression for binary classification
% Explain the loss function and optimization method

\subsubsection{Softmax Regression (Optional)}
% If multi-class: Describe Softmax Regression
% Explain how it extends logistic regression to multiple classes

\subsubsection{Decision Tree}
% Describe the Decision Tree algorithm
% Explain splitting criteria (Gini, entropy)

\subsubsection{Random Forest}
% Describe the Random Forest ensemble method
% Explain how it combines multiple decision trees

\subsubsection{Support Vector Machine}
% Describe SVM with linear and kernel-based approaches
% Explain the margin maximization concept

\subsection{Hyperparameter Tuning}
% Describe the cross-validation strategy used
% Explain which hyperparameters were tuned for each model
% Mention the search method (grid search, random search, etc.)


\section{Results}\label{sec:results}

\subsection{Model Performance}
% Present the performance metrics for each model
% Include tables with Accuracy, Precision, Recall, F1 Score

\subsection{Confusion Matrices}
% Display confusion matrices for each classifier

\subsection{ROC Curves and AUC}
% Present ROC curves and AUC scores
% Compare different models

\subsection{Training vs. Validation Performance}
% Show learning curves
% Discuss overfitting/underfitting observations

\subsection{Computational Cost (Optional)}
% Compare training times and computational requirements


\section{Comparative Analysis}\label{sec:analysis}

\subsection{Best Performing Models}
% Discuss which models performed best and why
% Relate performance to dataset characteristics

\subsection{Model Assumptions and Performance}
% Analyze how model assumptions influence performance
% Discuss cases where certain models excel or fail

\subsection{Overfitting Trade-off}
% Observations on bias-variance trade-off
% Discuss regularization effects

\subsection{Visualizations}
% Include visualizations such as:
% - Learning curves
% - Decision boundaries (if feasible)
% - Feature importance (for tree-based models)

\subsubsection{Learning Curves}
% Show and discuss learning curves

\subsubsection{Decision Boundaries}
% If applicable, show 2D decision boundaries

\subsubsection{Feature Importance}
% For Random Forest and Decision Trees, show feature importance

\end{document}